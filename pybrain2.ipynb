{
 "metadata": {
  "name": "",
  "signature": "sha256:8f8ce38d08ff257c39145b6b54aceb0a66ef084254e282249adba9ee978756d5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pybrain\n",
      "reload(pybrain)\n",
      "import json\n",
      "import pandas as pd\n",
      "import json\n",
      "from collections import defaultdict\n",
      "import numpy as np\n",
      "\n",
      "import pybrain.datasets\n",
      "reload(pybrain.datasets)\n",
      "import pybrain.supervised.trainers\n",
      "from pybrain.structure.connections import FullConnection\n",
      "from pybrain.structure.connections import CustomFullConnection\n",
      "from pybrain.utilities import percentError\n",
      "from pybrain.tools import validation\n",
      "import sys\n",
      "from random import shuffle\n",
      "reload(validation)\n",
      "import pickle\n",
      "# from validation import CrossValidator\n",
      "# from pybrain.structure.connections import IdentityConnection\n",
      "# from pybrain.structure.connections.connection import Connection\n",
      "\n",
      "# pathways = json.load(open('pathways/kegg_api_dict.txt'))\n",
      "# data = pd.DataFrame.from_csv('/users/ryan/softwareprojects/r/scaled_ GSE4588 .txt')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "functions to parse pathways"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "def get_pathway_dict(pathway_file, data):\n",
      "    new_kegg=[]\n",
      "    api_pathways=defaultdict(list)\n",
      "\n",
      "    with open(pathway_file) as f:\n",
      "        for line in f:\n",
      "            ids=line.strip().split('\\t')\n",
      "            if ids[1][4:] in data.index.values.tolist():\n",
      "                api_pathways[ids[0]].append(ids[1][4:])\n",
      "    print len(api_pathways)\n",
      "    api_pathways2=api_pathways.copy()\n",
      "    for key in api_pathways:\n",
      "        if not api_pathways[key]:\n",
      "            del api_pathways2[key]\n",
      "    print len(api_pathways2)\n",
      "    return dict(api_pathways2)\n",
      "\n",
      "#     outfile=open('kegg_api_dict.txt','wb')\n",
      "#     json.dump(api_pathways,outfile,indent=1,sort_keys=True)\n",
      "    \n",
      "\n",
      "def sync_data_pathways(data,pathways):\n",
      "    #reduces genes in dataframe to only genes in pathways\n",
      "    genes_in_paths=[]\n",
      "    for path in pathways:\n",
      "        genes_in_paths+=pathways[path]\n",
      "    genes_to_keep=list(set(genes_in_paths))\n",
      "    \n",
      "    data1=data.loc[genes_to_keep]\n",
      "    return data1\n",
      "\n",
      "\n",
      "\n",
      "# pathways = get_pathway_dict('pathways/kegg_api.txt',data)\n",
      "# print pathways\n",
      "# data = sync_data_pathways(data,pathways)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Functions to build networks"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def convertDataNeuralNetwork(x, y):\n",
      "    colx = 1 if len(np.shape(x))==1 else np.size(x, axis=1)\n",
      "    coly = 1 if len(np.shape(y))==1 else np.size(y, axis=1)\n",
      "    \n",
      "    fulldata = pybrain.datasets.ClassificationDataSet(colx,coly, nb_classes=2)\n",
      "    for d, v in zip(x, y):\n",
      "        fulldata.addSample(d, v)\n",
      "    \n",
      "    return fulldata\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def add_pathway_connections(network, first_layer, second_layer, data, pathways):\n",
      "    #now the tricky layer from the pathways database  \n",
      "    \n",
      "    entrez_dict = {} #entrez dict connects entrezids to the index of the feature vector\n",
      "    for i,entrez in enumerate(data.index.values.tolist()):\n",
      "        entrez_dict[entrez] = i\n",
      "\n",
      "    \n",
      "    for path_index, path in enumerate(pathways.keys()):\n",
      "        g_index=[]\n",
      "        for g in pathways[path]:\n",
      "            g_index.append(entrez_dict[g])\n",
      "\n",
      "        network.addConnection(CustomFullConnection(first_layer,second_layer,\\\n",
      "                                        inSliceIndices=g_index,  \\\n",
      "                                        outSliceIndices=[path_index]))\n",
      "    return network\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "## build structure\n",
      "def build_flat_network(data, pathways, layers=2, second_hidden=10):\n",
      "    #data is a data frame straight from affy - columns are patients, rows are entrez genes\n",
      "    #pathways is a dict with {pathway1:[gene1, gene2, ...], pathway2:[gene, gene...]...}\n",
      "\n",
      "    in_data = data.values.T\n",
      "\n",
      "    fnn = pybrain.structure.networks.FeedForwardNetwork()\n",
      "\n",
      "    inLayer = pybrain.structure.LinearLayer(in_data.shape[1])\n",
      "    fnn.addInputModule(inLayer)\n",
      "\n",
      "    outLayer = pybrain.structure.SoftmaxLayer(2)\n",
      "    fnn.addOutputModule(outLayer)\n",
      "\n",
      "    hidden_list=[]\n",
      "    #right now I have two sigmoid hidden layers \n",
      "    #can and will probably change\n",
      "    for i in range(layers):\n",
      "        if i ==0:\n",
      "            hidden_list.append(pybrain.structure.SigmoidLayer(len(pathways)))\n",
      "            fnn.addModule(hidden_list[i])\n",
      "        else:\n",
      "            hidden_list.append(pybrain.structure.SigmoidLayer(second_hidden))\n",
      "            fnn.addModule(hidden_list[i])\n",
      "    \n",
      "    \n",
      "    \n",
      "    ## add connections input to hidden is sparse, but second hidden and output are fully connected\n",
      "    \n",
      "    #add fully connected layers\n",
      "    hidden_to_out = pybrain.structure.connections.FullConnection(hidden_list[-1],outLayer)\n",
      "    fnn.addConnection(hidden_to_out)\n",
      "\n",
      "    \n",
      "#     hidden_connects=[]\n",
      "    for hl in range(1,layers):\n",
      "        #this first step may be unncessary, but I am saving the connections objects to a list incase I need them later\n",
      "#         hidden_connects.append(FullConnection(hidden_list[hl-1],hidden_list[hl]))\n",
      "        fnn.addConnection(FullConnection(hidden_list[hl-1],hidden_list[hl]))\n",
      "\n",
      "    #now the tricky layer from the pathways database\n",
      "    fnn = add_pathway_connections(fnn, inLayer, hidden_list[0], data, pathways)\n",
      "\n",
      "\n",
      "\n",
      "    fnn.sortModules()\n",
      "    return fnn\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def build_deep_network(data, pathways, filters=5, third_layer_nodes=5):\n",
      "    #data is a data frame straight from affy - columns are patients, rows are entrez genes\n",
      "    #pathways is a dict with {pathway1:[gene1, gene2, ...], pathway2:[gene, gene...]...}\n",
      "    \n",
      "    layers = filters\n",
      "    \n",
      "    in_data = data.values.T\n",
      "\n",
      "    fnn = pybrain.structure.networks.FeedForwardNetwork()\n",
      "\n",
      "    inLayer = pybrain.structure.LinearLayer(in_data.shape[1])\n",
      "    fnn.addInputModule(inLayer)\n",
      "\n",
      "    outLayer = pybrain.structure.SoftmaxLayer(2)\n",
      "    fnn.addOutputModule(outLayer)\n",
      "\n",
      "    hidden_list=[]\n",
      "    #right now I have two sigmoid hidden layers \n",
      "    #can and will probably change\n",
      "    for i in range(layers):\n",
      "        hidden_list.append(pybrain.structure.SigmoidLayer(len(pathways)))\n",
      "        fnn.addModule(hidden_list[i])\n",
      "    \n",
      "    clean_up_layer = pybrain.structure.SigmoidLayer(third_layer_nodes)\n",
      "    fnn.addModule(clean_up_layer)\n",
      "    \n",
      "    \n",
      "    \n",
      "    ## add connections input to hidden is sparse, but second hidden and output are fully connected\n",
      "    \n",
      "    #add fully connected layers\n",
      "    hidden_to_out = pybrain.structure.connections.FullConnection(clean_up_layer,outLayer)\n",
      "    fnn.addConnection(hidden_to_out)\n",
      "\n",
      "    \n",
      "#     hidden_connects=[]\n",
      "    for i in range(layers):\n",
      "        fnn.addConnection(FullConnection(hidden_list[i],clean_up_layer))\n",
      "        #now the tricky layer from the pathways database\n",
      "        fnn = add_pathway_connections(fnn, inLayer, hidden_list[i], data, pathways)\n",
      "        #this first step may be unncessary, but I am saving the connections objects to a list incase I need them later\n",
      "#         hidden_connects.append(FullConnection(hidden_list[hl-1],hidden_list[hl])\n",
      "\n",
      "    fnn.sortModules()\n",
      "    return fnn\n",
      "\n",
      "\n",
      "\n",
      "def shuffle_split(x,y,split_proportion=.8):\n",
      "    inds = range(len(y))\n",
      "    shuffle(inds)\n",
      "\n",
      "    x=x[inds,:]\n",
      "    y=y[inds]\n",
      "\n",
      "    split_proportion=.8\n",
      "    train_inds = int(round(split_proportion*x.shape[0]))\n",
      "\n",
      "    xtrain=x[:train_inds,:]\n",
      "    xtest = x[train_inds:, :]\n",
      "    ytrain=y[:train_inds]\n",
      "    ytest = y[train_inds:]\n",
      "    \n",
      "    return xtrain, xtest, ytrain, ytest"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Test code on samples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## load in raw data\n",
      "# tnfa_raw1 = pd.DataFrame.from_csv('/users/ryan/softwareprojects/R/tnfa_response_expression.txt')\n",
      "\n",
      "\n",
      "import CleanMetadata \n",
      "\n",
      "tnfa_raw, y = CleanMetadata.slice_and_clean()\n",
      "inds = [str(i) for i in tnfa_raw.index.values.tolist()]\n",
      "tnfa_raw.index = inds\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set up y vector (1 is responder, 0 is non-responder)\n",
      "# classes = []\n",
      "# for i in range(18):\n",
      "#     classes.append(1)\n",
      "    \n",
      "# for i in range(18,42):\n",
      "#     classes.append(0)\n",
      "\n",
      "# for i in range(42, 49):\n",
      "#     classes.append(0)\n",
      "    \n",
      "# for i in range(49,len(tnfa_raw.columns.values.tolist())):\n",
      "#     classes.append(1)\n",
      "# classes=[]\n",
      "# for i in range(100):\n",
      "#     classes.append(0)\n",
      "# for i in range(100,299):\n",
      "#     classes.append(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pathways = get_pathway_dict('pathways/kegg_api.txt',tnfa_raw)\n",
      "data = sync_data_pathways(tnfa_raw,pathways)\n",
      "fnn=build_flat_network(data, pathways)\n",
      "\n",
      "for m in fnn.modules:\n",
      "    print 'm', m\n",
      "#     print len(fnn.modules)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "295\n",
        "295\n",
        "m <SigmoidLayer 'SigmoidLayer-1218'>\n",
        "m <SigmoidLayer 'SigmoidLayer-1219'>\n",
        "m <LinearLayer 'LinearLayer-919'>\n",
        "m <SoftmaxLayer 'SoftmaxLayer-1220'>\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "quick test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# x=data.values.T\n",
      "# y=np.array(y)\n",
      "\n",
      "\n",
      "# Train = convertDataNeuralNetwork(xtrain, ytrain)\n",
      "# Test = convertDataNeuralNetwork(xtest, ytest)\n",
      "# Train._convertToOneOfMany()\n",
      "# Test._convertToOneOfMany()\n",
      "\n",
      "\n",
      "# trainer =  pybrain.supervised.trainers.BackpropTrainer(fnn, dataset=Train, verbose=False)\n",
      "\n",
      "\n",
      "# # print validation.CrossValidator(trainer, fulldata, n_folds=3,max_epochs=1).validate()\n",
      "# print fnn.activate(x[1,:])\n",
      "# trainer.trainEpochs( 10 )\n",
      "# print fnn.activate(x[1,:])\n",
      "\n",
      "# foo=1\n",
      "# pickle.dump(trainer, open('trainer_%s'%foo,'w'))\n",
      "# pickle.dump(fnn, open('trainer_%s'%foo,'w'))\n",
      "# trainer.train()\n",
      "\n",
      "fnn = pickle.load(open('trainer_%s'%foo))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# trainer.trainEpochs( 10 )\n",
      "print fnn.activate(x[1,:])\n",
      "print fnn.activate(x[2,:])\n",
      "print fnn.activate(x[3,:])\n",
      "print fnn.activate(x[4,:])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.10756919  0.89243081]\n",
        "[ 0.03152134  0.96847866]\n",
        "[ 0.24368298  0.75631702]\n",
        "[ 0.24406922  0.75593078]\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Main Script\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x=data.values.T\n",
      "y=np.array(y)\n",
      "xtrain, xtest, ytrain, ytest = shuffle_split(x, y)\n",
      "\n",
      "maxEpochs = 2000\n",
      "epochsperstep = 50\n",
      "learningrate = [.05, .01, 0.005, .001, .0005, .0001, .00005, .00001]\n",
      "hidden = [1, 5, 10, 50]\n",
      "deep_layers=[3,5,10,15]\n",
      "momentum = 0.\n",
      "noise=[.05, .03, .001, .003, .001]\n",
      "\n",
      "\n",
      "\n",
      "for lr in learningrate:\n",
      "    for n in noise:\n",
      "        result_count=0\n",
      "\n",
      "        print '\\n\\n\\nnoise =', n, \"learning rate =\",lr\n",
      "    #     print 'hidden nodes in second layer of flat = ', hidden_nodes\n",
      "\n",
      "\n",
      "        Train = convertDataNeuralNetwork(xtrain, ytrain)\n",
      "        Test = convertDataNeuralNetwork(xtest, ytest)\n",
      "        Train._convertToOneOfMany()\n",
      "        Test._convertToOneOfMany()\n",
      "\n",
      "        print('\\nFlat network')\n",
      "        fnn=build_flat_network(data, pathways, second_hidden=10)\n",
      "        trainer =  pybrain.supervised.trainers.BackpropTrainer(fnn, dataset=Train, verbose=False, \\\n",
      "                                                               learningrate = lr, lrdecay = 1, momentum = momentum)\n",
      "\n",
      "\n",
      "\n",
      "        with open('test_flat_network_lr%s_n%s.txt'%(lr,n),'w') as f:\n",
      "            f.write('learning rate='+str(lr)+'noise='+str(n)+'\\n')\n",
      "            f.write('epoch'+'\\t'+'train error'+'\\t'+'test error'+'\\n')\n",
      "\n",
      "            for i in range(1):\n",
      "                trainer.trainEpochs( 10 )\n",
      "                trnresult = percentError( trainer.testOnClassData(),\n",
      "                                          Train['class'] )\n",
      "                tstresult = percentError( trainer.testOnClassData(\n",
      "                       dataset=Test ), Test['class'] )\n",
      "\n",
      "                print \"epoch: %4d\" % trainer.totalepochs, \\\n",
      "                      \"  train error: %5.2f%%\" % trnresult, \\\n",
      "                      \"  test error: %5.2f%%\" % tstresult\n",
      "\n",
      "                f.write(str(trainer.totalepochs)+'\\t'+str(trnresult)+'\\t'+str(tstresult)+'\\n')\n",
      "\n",
      "                sys.stdout.flush()\n",
      "                if trnresult<=5:\n",
      "                    result_count+=1\n",
      "                    if result_count>5:\n",
      "                        break\n",
      "                else:\n",
      "                    result_count=0\n",
      "\n",
      "\n",
      "\n",
      "                xtrain_new=xtrain+np.random.normal(scale=n, size=xtrain.shape)\n",
      "                Train = convertDataNeuralNetwork(xtrain_new, ytrain)\n",
      "                Train._convertToOneOfMany()\n",
      "                trainer.setData(Train)\n",
      "        print \"dumping\"\n",
      "        pickle.dump(trainer, open('trainer_%s'%foo,'w'))\n",
      "        pickle.dump(fnn, open('trainer_%s'%foo,'w'))\n",
      "                \n",
      "                \n",
      "                \n",
      "        \"\"\"Deep Network\"\"\"\n",
      "\n",
      "#         fnn=build_deep_network(data, pathways, filters=10)\n",
      "#         # print fnn\n",
      "#         print('\\ndeep network')\n",
      "\n",
      "#         trainer =  pybrain.supervised.trainers.BackpropTrainer(fnn, dataset=Train, verbose=False, \\\n",
      "#                                                                learningrate = .1, lrdecay = 1, momentum = momentum)\n",
      "\n",
      "\n",
      "\n",
      "#         with open('test_deep_network_lr%s_n%s.txt'%(lr,n),'w') as f:\n",
      "#             f.write('learning rate='+str(lr)+'noise='+str(n)+'\\n')\n",
      "#             f.write('epoch'+'\\t'+'train error'+'\\t'+'test error'+'\\n')\n",
      "\n",
      "#             for i in range(100):\n",
      "#                 trainer.trainEpochs( 10 )\n",
      "#                 trnresult = percentError( trainer.testOnClassData(),\n",
      "#                                           Train['class'] )\n",
      "#                 tstresult = percentError( trainer.testOnClassData(\n",
      "#                        dataset=Test ), Test['class'] )\n",
      "\n",
      "#                 print \"epoch: %4d\" % trainer.totalepochs, \\\n",
      "#                       \"  train error: %5.2f%%\" % trnresult, \\\n",
      "#                       \"  test error: %5.2f%%\" % tstresult\n",
      "                        \n",
      "#                 sys.stdout.flush()\n",
      "#                 if trnresult<=5:\n",
      "#                     result_count+=1\n",
      "#                     if result_count>5:\n",
      "#                         break\n",
      "#                 else:\n",
      "#                     result_count=0\n",
      "\n",
      "\n",
      "\n",
      "#                 xtrain_new=xtrain+np.random.normal(scale=n, size=xtrain.shape)\n",
      "#                 Train = convertDataNeuralNetwork(xtrain_new, ytrain)\n",
      "#                 Train._convertToOneOfMany()\n",
      "#                 trainer.setData(Train)\n",
      "#                     # if trainer.totalepochs%40==0:\n",
      "#                         # print 'reshuffeling..'\n",
      "#                         # Train, Test = fulldata.splitWithProportion(.8)\n",
      "#                         # Train._convertToOneOfMany()\n",
      "#                         # Test._convertToOneOfMany()\n",
      "#                         # trainer.setData(Train)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "\n",
        "noise = 0.05 learning rate = 0.05\n",
        "\n",
        "Flat network\n",
        "epoch:   10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   train error: 39.41%   test error: 42.86%\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "dumping\n"
       ]
      },
      {
       "ename": "NameError",
       "evalue": "name 'pickle' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-15-a1d11c55bc40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"dumping\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainer_%s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mfoo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainer_%s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mfoo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for m in fnn.modules:\n",
      "#     print m\n",
      "#     print fnn.connections[m], '\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "foo = int(round(167*.8))\n",
      "print foo"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "134\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}